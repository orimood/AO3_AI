{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 324,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.015432098765432098,
      "grad_norm": 1.0455875396728516,
      "learning_rate": 4.938271604938271e-05,
      "loss": 2.9369,
      "step": 5
    },
    {
      "epoch": 0.030864197530864196,
      "grad_norm": 0.8370386958122253,
      "learning_rate": 4.8611111111111115e-05,
      "loss": 2.5359,
      "step": 10
    },
    {
      "epoch": 0.046296296296296294,
      "grad_norm": 0.9157159924507141,
      "learning_rate": 4.783950617283951e-05,
      "loss": 2.8571,
      "step": 15
    },
    {
      "epoch": 0.06172839506172839,
      "grad_norm": 0.998518168926239,
      "learning_rate": 4.70679012345679e-05,
      "loss": 2.5701,
      "step": 20
    },
    {
      "epoch": 0.07716049382716049,
      "grad_norm": 1.0039418935775757,
      "learning_rate": 4.62962962962963e-05,
      "loss": 2.5209,
      "step": 25
    },
    {
      "epoch": 0.09259259259259259,
      "grad_norm": 1.1499184370040894,
      "learning_rate": 4.5524691358024696e-05,
      "loss": 2.5617,
      "step": 30
    },
    {
      "epoch": 0.10802469135802469,
      "grad_norm": 1.1677817106246948,
      "learning_rate": 4.4753086419753084e-05,
      "loss": 2.6945,
      "step": 35
    },
    {
      "epoch": 0.12345679012345678,
      "grad_norm": 1.345494270324707,
      "learning_rate": 4.3981481481481486e-05,
      "loss": 2.5896,
      "step": 40
    },
    {
      "epoch": 0.1388888888888889,
      "grad_norm": 1.3907291889190674,
      "learning_rate": 4.3209876543209875e-05,
      "loss": 2.4628,
      "step": 45
    },
    {
      "epoch": 0.15432098765432098,
      "grad_norm": 1.4562612771987915,
      "learning_rate": 4.243827160493827e-05,
      "loss": 2.4853,
      "step": 50
    },
    {
      "epoch": 0.1697530864197531,
      "grad_norm": 1.4510300159454346,
      "learning_rate": 4.166666666666667e-05,
      "loss": 2.5723,
      "step": 55
    },
    {
      "epoch": 0.18518518518518517,
      "grad_norm": 1.4416706562042236,
      "learning_rate": 4.089506172839506e-05,
      "loss": 3.2706,
      "step": 60
    },
    {
      "epoch": 0.2006172839506173,
      "grad_norm": 1.0901368856430054,
      "learning_rate": 4.012345679012346e-05,
      "loss": 2.3342,
      "step": 65
    },
    {
      "epoch": 0.21604938271604937,
      "grad_norm": 1.260434865951538,
      "learning_rate": 3.935185185185186e-05,
      "loss": 2.6477,
      "step": 70
    },
    {
      "epoch": 0.23148148148148148,
      "grad_norm": 1.5482932329177856,
      "learning_rate": 3.8580246913580246e-05,
      "loss": 2.3864,
      "step": 75
    },
    {
      "epoch": 0.24691358024691357,
      "grad_norm": 1.4527236223220825,
      "learning_rate": 3.780864197530865e-05,
      "loss": 2.585,
      "step": 80
    },
    {
      "epoch": 0.2623456790123457,
      "grad_norm": 1.2533358335494995,
      "learning_rate": 3.7037037037037037e-05,
      "loss": 2.3447,
      "step": 85
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 1.758658528327942,
      "learning_rate": 3.626543209876543e-05,
      "loss": 2.2794,
      "step": 90
    },
    {
      "epoch": 0.2932098765432099,
      "grad_norm": 1.5005707740783691,
      "learning_rate": 3.5493827160493834e-05,
      "loss": 2.7687,
      "step": 95
    },
    {
      "epoch": 0.30864197530864196,
      "grad_norm": 1.1807066202163696,
      "learning_rate": 3.472222222222222e-05,
      "loss": 2.5191,
      "step": 100
    },
    {
      "epoch": 0.32407407407407407,
      "grad_norm": 1.1726895570755005,
      "learning_rate": 3.395061728395062e-05,
      "loss": 2.35,
      "step": 105
    },
    {
      "epoch": 0.3395061728395062,
      "grad_norm": 1.2445197105407715,
      "learning_rate": 3.317901234567901e-05,
      "loss": 2.5207,
      "step": 110
    },
    {
      "epoch": 0.3549382716049383,
      "grad_norm": 1.530730128288269,
      "learning_rate": 3.240740740740741e-05,
      "loss": 2.4416,
      "step": 115
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 1.4600714445114136,
      "learning_rate": 3.16358024691358e-05,
      "loss": 2.4439,
      "step": 120
    },
    {
      "epoch": 0.38580246913580246,
      "grad_norm": 1.7287471294403076,
      "learning_rate": 3.08641975308642e-05,
      "loss": 2.9888,
      "step": 125
    },
    {
      "epoch": 0.4012345679012346,
      "grad_norm": 1.3688255548477173,
      "learning_rate": 3.0092592592592593e-05,
      "loss": 2.4513,
      "step": 130
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 1.7520720958709717,
      "learning_rate": 2.9320987654320992e-05,
      "loss": 2.5105,
      "step": 135
    },
    {
      "epoch": 0.43209876543209874,
      "grad_norm": 1.4027581214904785,
      "learning_rate": 2.8549382716049384e-05,
      "loss": 2.4288,
      "step": 140
    },
    {
      "epoch": 0.44753086419753085,
      "grad_norm": 1.8533579111099243,
      "learning_rate": 2.777777777777778e-05,
      "loss": 2.2746,
      "step": 145
    },
    {
      "epoch": 0.46296296296296297,
      "grad_norm": 1.5526182651519775,
      "learning_rate": 2.700617283950617e-05,
      "loss": 2.2916,
      "step": 150
    },
    {
      "epoch": 0.4783950617283951,
      "grad_norm": 1.4736140966415405,
      "learning_rate": 2.623456790123457e-05,
      "loss": 2.3452,
      "step": 155
    },
    {
      "epoch": 0.49382716049382713,
      "grad_norm": 1.2913429737091064,
      "learning_rate": 2.5462962962962965e-05,
      "loss": 2.4134,
      "step": 160
    },
    {
      "epoch": 0.5092592592592593,
      "grad_norm": 1.5051331520080566,
      "learning_rate": 2.4691358024691357e-05,
      "loss": 2.4431,
      "step": 165
    },
    {
      "epoch": 0.5246913580246914,
      "grad_norm": 1.7218775749206543,
      "learning_rate": 2.3919753086419755e-05,
      "loss": 2.5309,
      "step": 170
    },
    {
      "epoch": 0.5401234567901234,
      "grad_norm": 1.4322781562805176,
      "learning_rate": 2.314814814814815e-05,
      "loss": 2.364,
      "step": 175
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 1.355372667312622,
      "learning_rate": 2.2376543209876542e-05,
      "loss": 2.2015,
      "step": 180
    },
    {
      "epoch": 0.5709876543209876,
      "grad_norm": 1.407377004623413,
      "learning_rate": 2.1604938271604937e-05,
      "loss": 2.2995,
      "step": 185
    },
    {
      "epoch": 0.5864197530864198,
      "grad_norm": 1.5328662395477295,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 2.502,
      "step": 190
    },
    {
      "epoch": 0.6018518518518519,
      "grad_norm": 53.13603210449219,
      "learning_rate": 2.006172839506173e-05,
      "loss": 2.941,
      "step": 195
    },
    {
      "epoch": 0.6172839506172839,
      "grad_norm": 2.081735372543335,
      "learning_rate": 1.9290123456790123e-05,
      "loss": 2.6033,
      "step": 200
    },
    {
      "epoch": 0.6327160493827161,
      "grad_norm": 1.4705387353897095,
      "learning_rate": 1.8518518518518518e-05,
      "loss": 2.1985,
      "step": 205
    },
    {
      "epoch": 0.6481481481481481,
      "grad_norm": 1.4590380191802979,
      "learning_rate": 1.7746913580246917e-05,
      "loss": 2.5482,
      "step": 210
    },
    {
      "epoch": 0.6635802469135802,
      "grad_norm": 1.5118979215621948,
      "learning_rate": 1.697530864197531e-05,
      "loss": 2.3948,
      "step": 215
    },
    {
      "epoch": 0.6790123456790124,
      "grad_norm": 28.14801025390625,
      "learning_rate": 1.6203703703703704e-05,
      "loss": 3.1357,
      "step": 220
    },
    {
      "epoch": 0.6944444444444444,
      "grad_norm": 1.6295127868652344,
      "learning_rate": 1.54320987654321e-05,
      "loss": 2.5077,
      "step": 225
    },
    {
      "epoch": 0.7098765432098766,
      "grad_norm": 1.6836960315704346,
      "learning_rate": 1.4660493827160496e-05,
      "loss": 2.2662,
      "step": 230
    },
    {
      "epoch": 0.7253086419753086,
      "grad_norm": 1.8865511417388916,
      "learning_rate": 1.388888888888889e-05,
      "loss": 2.4176,
      "step": 235
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 2.5435006618499756,
      "learning_rate": 1.3117283950617285e-05,
      "loss": 2.4265,
      "step": 240
    },
    {
      "epoch": 0.7561728395061729,
      "grad_norm": 31.03835105895996,
      "learning_rate": 1.2345679012345678e-05,
      "loss": 2.8537,
      "step": 245
    },
    {
      "epoch": 0.7716049382716049,
      "grad_norm": 2.4647862911224365,
      "learning_rate": 1.1574074074074075e-05,
      "loss": 2.7695,
      "step": 250
    },
    {
      "epoch": 0.7870370370370371,
      "grad_norm": 1.5359399318695068,
      "learning_rate": 1.0802469135802469e-05,
      "loss": 2.5152,
      "step": 255
    },
    {
      "epoch": 0.8024691358024691,
      "grad_norm": 1.413454294204712,
      "learning_rate": 1.0030864197530866e-05,
      "loss": 2.2553,
      "step": 260
    },
    {
      "epoch": 0.8179012345679012,
      "grad_norm": 1.740378499031067,
      "learning_rate": 9.259259259259259e-06,
      "loss": 2.4271,
      "step": 265
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 2.1564481258392334,
      "learning_rate": 8.487654320987654e-06,
      "loss": 2.4302,
      "step": 270
    },
    {
      "epoch": 0.8487654320987654,
      "grad_norm": 1.5284247398376465,
      "learning_rate": 7.71604938271605e-06,
      "loss": 2.3117,
      "step": 275
    },
    {
      "epoch": 0.8641975308641975,
      "grad_norm": 1.4785571098327637,
      "learning_rate": 6.944444444444445e-06,
      "loss": 2.5809,
      "step": 280
    },
    {
      "epoch": 0.8796296296296297,
      "grad_norm": 1.5733858346939087,
      "learning_rate": 6.172839506172839e-06,
      "loss": 2.5747,
      "step": 285
    },
    {
      "epoch": 0.8950617283950617,
      "grad_norm": 1.7073719501495361,
      "learning_rate": 5.401234567901234e-06,
      "loss": 2.2752,
      "step": 290
    },
    {
      "epoch": 0.9104938271604939,
      "grad_norm": 1.3289982080459595,
      "learning_rate": 4.6296296296296296e-06,
      "loss": 2.3568,
      "step": 295
    },
    {
      "epoch": 0.9259259259259259,
      "grad_norm": 1.9403737783432007,
      "learning_rate": 3.858024691358025e-06,
      "loss": 2.2753,
      "step": 300
    },
    {
      "epoch": 0.941358024691358,
      "grad_norm": 1.5352802276611328,
      "learning_rate": 3.0864197530864196e-06,
      "loss": 2.4457,
      "step": 305
    },
    {
      "epoch": 0.9567901234567902,
      "grad_norm": 1.7056219577789307,
      "learning_rate": 2.3148148148148148e-06,
      "loss": 2.4597,
      "step": 310
    },
    {
      "epoch": 0.9722222222222222,
      "grad_norm": 2.4272897243499756,
      "learning_rate": 1.5432098765432098e-06,
      "loss": 2.4311,
      "step": 315
    },
    {
      "epoch": 0.9876543209876543,
      "grad_norm": 1.7712652683258057,
      "learning_rate": 7.716049382716049e-07,
      "loss": 2.555,
      "step": 320
    }
  ],
  "logging_steps": 5,
  "max_steps": 324,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1204521548120064.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
